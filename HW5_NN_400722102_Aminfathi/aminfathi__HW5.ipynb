{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StudentName__HW5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Environment"
      ],
      "metadata": {
        "id": "Nd4GU-Pf4vd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "from gym import Env\n",
        "import datetime\n",
        "\n",
        "class FrozenLake(Env):\n",
        "    def __init__(self,studentNum:int=256, nonStationary = False):\n",
        "        self.studentNum = studentNum\n",
        "        self.nonStationary = nonStationary\n",
        "        \n",
        "        np.random.seed(self.studentNum)\n",
        "        self.beginMap = make_map(self.studentNum) #*2\n",
        "        self.beginMap[self.beginMap>1] = 1\n",
        "        self.endMap = make_map(self.studentNum + 100)\n",
        "        \n",
        "        self.changeDir = self.endMap - self.beginMap\n",
        "        self.changeDir *= 1/11000\n",
        "\n",
        "        self.fixedMap = self.beginMap\n",
        "\n",
        "        np.random.seed(datetime.datetime.now().microsecond)\n",
        "        \n",
        "        self.map = copy.deepcopy(self.fixedMap)\n",
        "        self.time = 0\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.NSreset()\n",
        "        if not self.nonStationary:\n",
        "            self.map = copy.deepcopy(self.fixedMap)\n",
        "            self.time = 0\n",
        "\n",
        "        return self.state\n",
        "\n",
        "    def NSreset(self):\n",
        "        self.time += 1\n",
        "        self.map += self.changeDir\n",
        "\n",
        "        self.map[self.map>0.95]=0.95\n",
        "        self.map[self.map<0.0]=0.0\n",
        "\n",
        "        self.state = (0,0)\n",
        "        self.done = False\n",
        "        return self.state\n",
        "    \n",
        "    def states_transitions(self, state, action):\n",
        "        x = state[0]\n",
        "        y = state[1]\n",
        "        states = np.array([[x,y-1], [x,y+1], [x-1 ,y], [x+1,y] ])\n",
        "\n",
        "\n",
        "        if action == UP:\n",
        "            selected = states[2]\n",
        "        if action == DOWN:\n",
        "            selected = states[3]\n",
        "        if action == RIGHT:\n",
        "            selected = states[1]\n",
        "        if action == LEFT:\n",
        "            selected = states[0]\n",
        "\n",
        "        zero = np.zeros((4,2)).astype(int)\n",
        "        three = (3 * np.ones((4,2))).astype(int)\n",
        "        output = np.maximum(np.minimum(states, three),zero)\n",
        "        output, indices = np.unique(output, axis = 0, return_counts= True)\n",
        "\n",
        "        \n",
        "        selected = np.maximum(np.minimum(selected, three[0]), zero[0])\n",
        "        probs = indices * 0.025\n",
        "        probs[np.argmax(np.sum(selected == output, axis = 1))] += 0.9\n",
        "\n",
        "        return list(zip(output[:,0],output[:,1])), probs\n",
        "    \n",
        "    def possible_consequences(self,action:int,state_now=None):\n",
        "\n",
        "        if state_now==None:\n",
        "            state_now = self.state\n",
        "\n",
        "        state = [state_now[0],state_now[1]]\n",
        "        states, probs = self.states_transitions(state, action)\n",
        "        aa = np.array(states) \n",
        "        fail_probs = self.map[(aa[:,0]),(aa[:,1])]\n",
        "        dones = np.sum(aa == 3, axis = 1) == 2\n",
        "        return states, probs, fail_probs,dones\n",
        "    \n",
        "    def step(self, a:int):\n",
        "        if not (a in range(4)):\n",
        "            raise Exception(\"action is not available!!!\")\n",
        "        \n",
        "        states, probs, fail_probs,dones = self.possible_consequences(a)\n",
        "        \n",
        "        next_idx = np.random.choice(np.arange(len(states)), p = probs)\n",
        "        next_state = states[next_idx]\n",
        "        self.state = tuple(next_state)\n",
        "        \n",
        "        self.done = dones[next_idx]\n",
        "\n",
        "        r = -1\n",
        "\n",
        "        if self.done:\n",
        "            r += 60\n",
        "        elif np.random.rand()< fail_probs[next_idx]:\n",
        "            r -= 15\n",
        "            self.done = True\n",
        "\n",
        "        return (self.state, r, self.done, {})\n",
        "\n",
        "    def render(self,state=None):\n",
        "        if state == None:\n",
        "            state = self.state\n",
        "\n",
        "        out = \"\"\n",
        "        for i in range(4):\n",
        "            out += \"\\n------------------------------\\n| \"\n",
        "            for j in range(4):\n",
        "                if (i,j) == state:\n",
        "                    out += \"\\033[44m{:.3f}\\033[0m | \".format(self.map[i,j])\n",
        "                else :\n",
        "                    out += \"{:.3f} | \".format(self.map[i,j])\n",
        "\n",
        "        out += \"\\n------------------------------\"\n",
        "        print(out)\n",
        "\n",
        "    def environment_states(self):\n",
        "        env_states = []\n",
        "        for state_index in range(16):\n",
        "            s0 = state_index % 4\n",
        "            s1 = state_index//4\n",
        "            env_states.append((s0,s1))\n",
        "        return env_states\n",
        "\n",
        "        \n",
        "def set_max_min(var,maximum,minimum):\n",
        "    return min(max(var,minimum),maximum)\n",
        "\n",
        "def make_map(studentNum):\n",
        "    np.random.seed(studentNum)  \n",
        "    move = np.zeros(6)\n",
        "    idx = np.random.choice(range(6),size=3,replace=False)\n",
        "    move[idx] = 1\n",
        "\n",
        "    point = [0,0]\n",
        "    lowprobs = [tuple(point)]\n",
        "\n",
        "    for m in move:\n",
        "        if m:\n",
        "            point[0] += 1\n",
        "        else:\n",
        "            point[1] += 1\n",
        "        lowprobs.append(tuple(point))\n",
        "    \n",
        "    map = np.random.rand(4,4)\n",
        "    idx = np.array(lowprobs)\n",
        "\n",
        "    map[idx[:,0],idx[:,1]] = 0.001 \n",
        "    map[0,0] = 0.0\n",
        "    map[3,3] = 0.0 \n",
        "\n",
        "    return map"
      ],
      "metadata": {
        "id": "b8Ie-mkhiGjT"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Your Student ID"
      ],
      "metadata": {
        "id": "JIrM2PNQ4l5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "STUDENT_NUM = 400722102"
      ],
      "metadata": {
        "id": "JvWeNrBx4or9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HyperParameters"
      ],
      "metadata": {
        "id": "ba-NtxEn5LJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#%% allowed actions\n",
        "LEFT = 0\n",
        "DOWN = 1\n",
        "RIGHT = 2\n",
        "UP = 3\n",
        "\n",
        "ACTIONS = [LEFT,DOWN,RIGHT,UP]\n",
        "\n",
        "#%% hyperparameters\n",
        "EPISODES = 10000\n",
        "EPSILON = 0.1\n",
        "LEARNING_RATE = 0.1\n",
        "DISCOUNT = 0.9"
      ],
      "metadata": {
        "id": "54H4VswF5Kot"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Map of environment"
      ],
      "metadata": {
        "id": "Mdub8jub5bM9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ME5gllo7g0p7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d20f0090-1f12-4f3c-d5c9-7673ec942c9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment with fail probabilities :\n",
            "\n",
            "------------------------------\n",
            "| \u001b[44m0.000\u001b[0m | 0.001 | 0.086 | 0.517 | \n",
            "------------------------------\n",
            "| 0.412 | 0.001 | 0.563 | 0.435 | \n",
            "------------------------------\n",
            "| 0.134 | 0.001 | 0.001 | 0.001 | \n",
            "------------------------------\n",
            "| 0.005 | 0.229 | 0.687 | 0.000 | \n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "environment = FrozenLake(studentNum=STUDENT_NUM)\n",
        "\n",
        "print(\"Environment with fail probabilities :\")\n",
        "environment.render()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <h2><font color=indigo> Agent Implementation\n",
        "Implement your q-learning (off-policy TD) agent here. You need to utilize the step function provided in the Environment class to interact with frozen lake environment."
      ],
      "metadata": {
        "id": "9z-KEpaeOcAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import itertools\n",
        "import random\n",
        "\n",
        "class Q_Learning:\n",
        "    def __init__(self, id, environment, discount , learning_rate = 0.1 , epsilon = 0.1 ,episodes=10000):\n",
        "\n",
        "        self.environment = environment\n",
        "        self.discount = discount\n",
        "        self.episodes = episodes\n",
        "        self.id = id\n",
        "        self.learning_rate = learning_rate\n",
        "        self.environment = environment\n",
        "        self.epsilon = epsilon\n",
        "        self.action_size = 4\n",
        "        self.state_size = 16\n",
        "        self.map_size = 16\n",
        "        self.map_y = 4\n",
        "\n",
        "\n",
        "        # Create our Q table with state_size rows and action_size columns (16x4)\n",
        "        self.qtable = np.zeros((self.state_size, self.action_size))\n",
        "\n",
        "          # convert states into 0 to max_size number (15 in this example)\n",
        "    def State_Number(self, state):\n",
        "        return self.map_y*state[0] + state[1]\n",
        "\n",
        "    def Witch_action(self , state ):\n",
        "      cstate = self.State_Number(state)\n",
        "      #Choose an action a in the current world state (s)\n",
        "      ## First we randomize a number\n",
        "      exp_exp_tradeoff = random.uniform(0, 1)\n",
        "      ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
        "      if exp_exp_tradeoff > self.epsilon:\n",
        "            action = np.argmax(self.qtable[cstate,:])\n",
        "            #print(exp_exp_tradeoff, \"action\", action)\n",
        "      # Else doing a random choice --> exploration\n",
        "      else:\n",
        "            action =  action = np.random.choice(self.action_size)\n",
        "      return action \n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "    # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "    # qtable[new_state,:] : all the actions we can take from new state\n",
        "    def update(self ,action, state, newstate, reward ):\n",
        "      cstate = self.State_Number(state)\n",
        "      new_state = self.State_Number(newstate)\n",
        "      max_Q_next_state = self.qtable[new_state][np.argmax(self.qtable[new_state])]\n",
        "      new_Q = self.qtable[cstate][action] + self.learning_rate * (reward + self.discount *max_Q_next_state - self.qtable[cstate][action])\n",
        "\n",
        "      self.qtable[cstate][action] = new_Q\n",
        "\n",
        "\n",
        "    # train the agent and return rewards for the given number of episodes\n",
        "    def train(self):\n",
        "      # List of rewards\n",
        "      rewards = np.zeros(self.episodes)\n",
        "      # 2 For life or until learning is stopped\n",
        "      for episode in range(self.episodes):\n",
        "      # Reset the environment\n",
        "            state = self.environment.reset()\n",
        "            step = 0\n",
        "            done = False\n",
        "            total_rewards = 0\n",
        "            current_state = self.environment.reset()\n",
        "\n",
        "            while (done == False) :\n",
        "\n",
        "                bestAction = self.Witch_action(current_state)\n",
        "                next_state, reward, done, _ = environment.step(bestAction)\n",
        "                self.update(bestAction, current_state, next_state, reward)\n",
        "                total_rewards += reward\n",
        "                current_state = next_state         \n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "            rewards[episode] = total_rewards\n",
        "            \n",
        "      return rewards\n",
        "\n",
        "        # print and return policy (greedy policy based on Q values)    \n",
        "    def print(self):\n",
        "        actions = [\"LEFT\",\"DOWN\",\"RIGHT\",\"UP\"]\n",
        "        policy = []\n",
        "        policy_string = []\n",
        "        \n",
        "        for i in range(len(self.qtable)-1):\n",
        "            best_Q = np.argmax(self.qtable[i])\n",
        "            policy.append(best_Q)\n",
        "            action = actions[best_Q]\n",
        "            policy_string.append(action)\n",
        "            \n",
        "\n",
        "            print( i , \"Best Action: \", action)\n",
        "            \n",
        "        return policy, policy_string"
      ],
      "metadata": {
        "id": "9P-5IZqIeco8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <h2><font color=indigo> Q Values\n",
        "Return the Q values that your agent learns in here:"
      ],
      "metadata": {
        "id": "c6J4uXQGuTgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Q_Learning('aminfathi', environment, 0.9 , learning_rate = 0.5 , epsilon = 0.1 ,episodes= 10000)\n",
        "train = agent.train()\n"
      ],
      "metadata": {
        "id": "WYZfiWY6qMch"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q= agent.qtable\n",
        "print(q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qt1WRnFAiqNy",
        "outputId": "159d2962-b6d6-4aec-97fc-ff246b1d3809"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[19.55073099 21.25117862 29.70987633 20.27781071]\n",
            " [22.55640762 34.27277799 18.37572447 24.33366776]\n",
            " [24.1196412  12.85833291 -3.41919757  9.57525493]\n",
            " [-3.94785194 -0.5        -0.5        -0.5       ]\n",
            " [12.43778105 23.52044744 27.48682397 16.95707254]\n",
            " [20.88703845 39.79472378 21.19381667 27.06192036]\n",
            " [22.09683577 23.67546369 35.13202156 14.71707914]\n",
            " [15.14200032 48.48290638 22.5715067  -3.74485665]\n",
            " [26.57833206 26.63068419 33.60781843 17.49265789]\n",
            " [31.52336654 31.96802675 45.83798053 30.63466927]\n",
            " [36.29993423 35.20499579 52.0920756  25.7943259 ]\n",
            " [45.58103957 58.99938453 46.14096907 41.96521272]\n",
            " [ 7.52030262  5.97761992 32.63835064 17.65312106]\n",
            " [24.21896353 31.82155726 36.91914617 29.06706686]\n",
            " [32.67412772 35.12168548 58.98029954 39.03929948]\n",
            " [ 0.          0.          0.          0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "environment.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0chTR4vi-0q",
        "outputId": "4acf6414-baa1-4ff6-cf6f-cbfb9e930d2a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------------------------------\n",
            "| 0.000 | 0.001 | 0.086 | 0.517 | \n",
            "------------------------------\n",
            "| 0.412 | 0.001 | 0.563 | 0.435 | \n",
            "------------------------------\n",
            "| 0.134 | 0.001 | 0.001 | 0.001 | \n",
            "------------------------------\n",
            "| 0.005 | 0.229 | 0.687 | \u001b[44m0.000\u001b[0m | \n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <h2><font color=darkcyan> Policy\n",
        "Return the optimal policy that your agent learns in here:"
      ],
      "metadata": {
        "id": "MMojYRGVvAXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy, policy_string = agent.print()"
      ],
      "metadata": {
        "id": "9EFY3T0r9OHW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e9ffbcf-159c-4d2b-8ce0-56977042d2b5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 Best Action:  RIGHT\n",
            "1 Best Action:  DOWN\n",
            "2 Best Action:  LEFT\n",
            "3 Best Action:  DOWN\n",
            "4 Best Action:  RIGHT\n",
            "5 Best Action:  DOWN\n",
            "6 Best Action:  RIGHT\n",
            "7 Best Action:  DOWN\n",
            "8 Best Action:  RIGHT\n",
            "9 Best Action:  RIGHT\n",
            "10 Best Action:  RIGHT\n",
            "11 Best Action:  DOWN\n",
            "12 Best Action:  RIGHT\n",
            "13 Best Action:  RIGHT\n",
            "14 Best Action:  RIGHT\n"
          ]
        }
      ]
    }
  ]
}